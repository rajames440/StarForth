////
StarForth Tier I Acceptance Criteria

Document Metadata:
- Document ID: starforth-governance/tier-i-acceptance-criteria
- Version: 1.0.0
- Created: 2025-10-25
- Purpose: Define specific pass/fail criteria for Tier I
- Scope: Measurable acceptance rules
- Status: REQUIREMENTS
////

= StarForth Tier I: Acceptance Criteria

**Version:** 1.0.0
**Status:** Active
**Last Updated:** October 25, 2025

---

== Overview

This document defines **exactly what Tier I passing looks like**. No ambiguity.

Every criterion is measurable. Every criterion is verifiable. Every criterion has a pass/fail condition.

---

## Criterion T1-1: FORTH-79 Compliance

=== Criterion T1-1.1: Core Word Implementation

**What:** Every FORTH-79 core word is implemented.

**Measurement:**
- Count of core words implemented / 70 = %
- Minimum 70/70 = 100%

**Pass Condition:**
- [ ] All 70 FORTH-79 core words have implementations
- [ ] No word marked "TODO" or "stub"
- [ ] Each word maps to specific function in code

**Evidence:**
- `FORTH-79_COMPLIANCE_MATRIX.adoc` shows 100% implementation
- Git log shows all words implemented

---

=== Criterion T1-1.2: Test Coverage for Each Word

**What:** Each word has test cases (functional, edge case, stress).

**Measurement:**
- Words with >=1 test / 70 = %
- Minimum 70/70 = 100%

**Pass Condition:**
- [ ] All 70 words have at least 1 test
- [ ] All 70 words have edge case coverage
- [ ] All 70 words have stress test (if applicable)
- [ ] Test IDs linked in COMPLIANCE_MATRIX

**Evidence:**
- `FORTH-79_COMPLIANCE_MATRIX.adoc` shows test IDs for each word
- Test code shows comprehensive coverage

---

=== Criterion T1-1.3: Multi-Platform Validation

**What:** All tests pass on multiple platforms.

**Measurement:**
- Platforms where all tests pass / minimum required

**Minimum Required Platforms:**
- [ ] Linux x86_64 (GCC)
- [ ] Linux ARM64 (GCC)
- [Optional] Fiasco.OC
- [Optional] seL4

**Pass Condition:**
- [ ] 675+ tests pass on Linux x86_64
- [ ] 675+ tests pass on Linux ARM64
- [ ] No platform-specific failures (same code, all platforms)
- [ ] Test results logged with date, platform, compiler version

**Evidence:**
- `TEST_RESULTS/TEST_RESULTS_LOG.adoc` shows pass on both platforms
- CI logs show consistent passes

---

=== Criterion T1-1.4: FORTH-79 Compliance Matrix Complete

**What:** `FORTH-79_COMPLIANCE_MATRIX.adoc` is complete and current.

**Pass Condition:**
- [ ] Every word has an entry
- [ ] Implementation location documented (file + function)
- [ ] Stack effect documented (e.g., n -- n+1)
- [ ] Test ID(s) linked
- [ ] No deviations from spec (or deviations documented with rationale)
- [ ] Document reviewed and approved

**Evidence:**
- Document exists and is complete

---

## Criterion T1-2: Requirements Traceability

=== Criterion T1-2.1: Requirements Specification Exists

**What:** Formal specification of StarForth as a document.

**Pass Condition:**
- [ ] `REQUIREMENTS_SPECIFICATION.adoc` exists
- [ ] Defines what StarForth is (FORTH-79 VM)
- [ ] Lists 70 core word requirements
- [ ] States quality requirements (determinism, memory safety, portability)
- [ ] Each requirement has unique ID (REQ-001, REQ-002, etc.)
- [ ] Document is reviewed and approved

**Evidence:**
- Document exists, is complete, and reviewed

---

=== Criterion T1-2.2: Requirements Traceability Matrix (RTM)

**What:** Bidirectional mapping of Requirements ↔ Code ↔ Tests.

**Pass Condition:**
- [ ] RTM exists as spreadsheet or table
- [ ] Every REQ → Implementation module (file + function)
- [ ] Every REQ → Test case (test ID)
- [ ] Every Test → REQ (no orphan tests)
- [ ] Every Code module → REQ (no orphan code)
- [ ] Coverage = 100% (all mapped, nothing missing)
- [ ] RTM reviewed and approved

**Measurement:**
- Orphan code count = 0
- Orphan test count = 0
- RTM completeness = 100%

**Evidence:**
- RTM document/spreadsheet with 100% mapping

---

=== Criterion T1-2.3: Code-to-Requirement Documentation

**What:** For each major module/function, what requirement does it implement?

**Pass Condition:**
- [ ] Each of 19 modules documented with: purpose, requirements, functions
- [ ] Each function >50 LOC has comment explaining what requirement it serves
- [ ] No mysterious code blocks without explanation
- [ ] Design decisions documented (why this way?)

**Measurement:**
- Modules documented / 19 = %
- Minimum 19/19 = 100%

**Evidence:**
- Module catalog document
- Code comments in git repository

---

## Criterion T1-3: Architecture & Design Documentation

=== Criterion T1-3.1: Architecture Overview Document

**What:** High-level view of how StarForth works.

**Pass Condition:**
- [ ] `ARCHITECTURE_OVERVIEW.adoc` exists
- [ ] Explains what StarForth is and what it does
- [ ] Shows 19 modules and how they interact
- [ ] Explains data flow (word lookup → execution)
- [ ] Explains control flow (interpreter loop)
- [ ] Includes diagrams or flow charts
- [ ] Document reviewed and approved

**Evidence:**
- Document complete and reviewed

---

=== Criterion T1-3.2: All 19 Modules Documented

**What:** `WORD_MODULE_CATALOG.adoc` describes every module.

**Pass Condition:**
- [ ] All 19 modules listed
- [ ] For each module: purpose, words, key algorithms, dependencies
- [ ] No duplicate or conflicting module descriptions
- [ ] Cross-references between modules clear
- [ ] Test coverage noted for each module
- [ ] Document reviewed and approved

**Measurement:**
- Modules documented / 19 = %
- Minimum 19/19 = 100%

**Evidence:**
- Module catalog document complete

---

=== Criterion T1-3.3: Design Decisions Documented

**What:** `DESIGN_DECISIONS.adoc` explains major architectural choices.

**Pass Condition:**
- [ ] Document exists and is complete
- [ ] For each major decision: what was decided, alternatives considered, why this choice, tradeoffs
- [ ] Examples: fixed 5MB arena, direct-threaded interpreter, 19 modules, dictionary-based isolation
- [ ] Rationale is clear (not just "felt right")
- [ ] Tradeoffs explicitly discussed (speed vs. safety, memory vs. flexibility, etc.)
- [ ] Document reviewed and approved

**Evidence:**
- Document exists and covers major decisions

---

=== Criterion T1-3.4: Platform Abstraction Layer Documented

**What:** How does StarForth handle multiple platforms (Linux, Fiasco.OC, seL4)?

**Pass Condition:**
- [ ] `PLATFORM_ABSTRACTION_LAYER.adoc` exists
- [ ] Explains layer architecture (what's platform-specific vs. portable)
- [ ] Linux/POSIX layer documented
- [ ] Fiasco.OC/L4Re layer documented
- [ ] seL4 layer documented (or marked TBD)
- [ ] Minimal/bare-metal layer documented
- [ ] Shows how same code runs on all platforms
- [ ] Document reviewed and approved

**Evidence:**
- Document exists and is complete

---

## Criterion T1-4: Test Methodology

=== Criterion T1-4.1: Test Plan Document

**What:** `TEST_PLAN.adoc` describes testing strategy.

**Pass Condition:**
- [ ] Document exists
- [ ] Defines what is being tested (FORTH-79 compliance)
- [ ] Defines test types (unit, integration, stress, edge case)
- [ ] Defines test organization (by module, by standard)
- [ ] Defines acceptance criteria (100% core words pass, all platforms)
- [ ] Defines test execution (CI/CD, release gates, manual steps)
- [ ] Defines defect reporting
- [ ] Document reviewed and approved

**Evidence:**
- Document complete and reviewed

---

=== Criterion T1-4.2: Test Cases Specified

**What:** `TEST_CASE_SPECIFICATION.adoc` defines each test precisely.

**Pass Condition:**
- [ ] Document exists
- [ ] For each test category (FORTH79-TEST-ADD, etc.):
  - [ ] Test ID documented
  - [ ] Purpose (what does it test?)
  - [ ] Inputs documented
  - [ ] Expected output documented
  - [ ] Edge cases listed
  - [ ] Pass/fail condition clear
- [ ] No ambiguity in any test specification
- [ ] Reviewable and reproducible by independent party
- [ ] Document reviewed and approved

**Evidence:**
- Test specification document complete

---

=== Criterion T1-4.3: Code Coverage Measured

**What:** Percentage of code executed by tests.

**Measurement:**
- Line coverage >= 90%
- Branch coverage >= 80%
- Function coverage >= 95%

**Pass Condition:**
- [ ] Coverage metrics generated (via gcov, lcov, or equivalent)
- [ ] Report shows >= 90% line coverage
- [ ] Report shows >= 80% branch coverage
- [ ] Report shows >= 95% function coverage
- [ ] Low-coverage areas identified and justified (if applicable)
- [ ] Report signed and dated

**Evidence:**
- Coverage report with metrics

---

=== Criterion T1-4.4: Test Results Logged

**What:** Every test run is logged with full reproducibility data.

**Pass Condition:**
- [ ] For each test run:
  - [ ] Date/time of test
  - [ ] Platform (OS, CPU, compiler version, compiler flags)
  - [ ] Test command (exact command to reproduce)
  - [ ] Git commit hash
  - [ ] Pass/fail result for each test
  - [ ] Total pass rate (%)
  - [ ] Any failures documented with details
  - [ ] Full test log archived
- [ ] Latest test results logged before Tier I sign-off
- [ ] All platforms tested in latest run
- [ ] 100% pass rate on all platforms

**Evidence:**
- `TEST_RESULTS/TEST_RESULTS_LOG.adoc` with latest runs

---

## Overall Tier I Acceptance Checklist

=== T1-1: FORTH-79 Compliance

- [ ] T1-1.1: All 70 core words implemented
- [ ] T1-1.2: All 70 words have test coverage
- [ ] T1-1.3: Tests pass on 2+ platforms
- [ ] T1-1.4: FORTH-79 Compliance Matrix complete

**Gate:** All four sub-criteria met = T1-1 PASS

---

=== T1-2: Requirements Traceability

- [ ] T1-2.1: Requirements Specification complete
- [ ] T1-2.2: Requirements Traceability Matrix 100% complete
- [ ] T1-2.3: Code-to-requirement documentation complete

**Gate:** All three sub-criteria met = T1-2 PASS

---

=== T1-3: Architecture & Design Documentation

- [ ] T1-3.1: Architecture Overview complete
- [ ] T1-3.2: All 19 modules documented
- [ ] T1-3.3: Design decisions documented with rationale
- [ ] T1-3.4: Platform abstraction layer documented

**Gate:** All four sub-criteria met = T1-3 PASS

---

=== T1-4: Test Methodology

- [ ] T1-4.1: Test Plan complete
- [ ] T1-4.2: Test Case specifications complete
- [ ] T1-4.3: Code coverage >=90% / >=80% / >=95%
- [ ] T1-4.4: Test results logged with 100% pass rate

**Gate:** All four sub-criteria met = T1-4 PASS

---

## Tier I Final Acceptance

**Tier I is PASS when:**
- ✓ T1-1 PASS
- ✓ T1-2 PASS
- ✓ T1-3 PASS
- ✓ T1-4 PASS
- ✓ Deficiency count <=5 (all non-critical)
- ✓ All deficiencies have remediation plan or documented waiver
- ✓ All signatures obtained (Validation Engineer, Review Authority, Maintainer)

**Tier I is FAIL if:**
- ✗ Any critical deficiency unresolved
- ✗ Any major criterion not met
- ✗ Code coverage <90%
- ✗ Tests failing on any platform
- ✗ Documentation incomplete

---

## Sign-Off

Tier I is certified complete only when all three sign:

| Role | Sign-Off | Date | Signature |
|------|----------|------|-----------|
| Validation Engineer | "I executed all protocols per procedure" | _____ | _____________ |
| Review Authority | "I reviewed all evidence and confirm Tier I PASS" | _____ | _____________ |
| Maintainer | "I accept Tier I validation and approve Tier II commencement" | _____ | _____________ |

---

## Document History

[cols="^1,^2,2,<4"]
|===
| Version | Date | Author | Change Summary

| 1.0.0
| 2025-10-25
| rajames
| Created Tier I acceptance criteria
|===

---

**Next:** Follow `VALIDATION_ENGINEERING_PLAN.adoc` to execute Tier I.

---

== Document Approval & Signature

[cols="2,2,1"]
|===
| Role | Name/Title | Signature

| **Author/Maintainer**
| Robert A. James
|

| **Date Approved**
|
| _______________

| **PGP Fingerprint**
| 497CF5C0D295A7E8065C5D9A9CD3FBE66B5E2AE4
|

|===

**PGP Signature Block:**
```
-----BEGIN PGP SIGNATURE-----

[Your PGP signature here - generated via: gpg --clearsign TIER_I_ACCEPTANCE_CRITERIA.adoc]

-----END PGP SIGNATURE-----
```

**To Sign This Document:**
```bash
gpg --clearsign TIER_I_ACCEPTANCE_CRITERIA.adoc
# This creates TIER_I_ACCEPTANCE_CRITERIA.adoc.asc (signed version)
```

**To Verify Signature:**
```bash
gpg --verify TIER_I_ACCEPTANCE_CRITERIA.adoc.asc
```

---

**StarForth Tier I:** Clear criteria. Binary pass/fail. No ambiguity.